#!/usr/bin/env python

""" moku-wiki

Converts a folder of Markdown files, applying the following transforms:

*  Inter-page links can be specified using the target page's title (as specified in its YAML
metadata block), e.g. '[[A Page Title]]'. This is converted to a standard Markdown link to the
HTML version of that page: '[A Page Title](a_page_title.html)'.

*  The YAML metadata can also have an "alias" field which can be used to link to that page instead
of the title. This can be useful if the actual title that is to be displayed (the "formal" title,
if you will) is long but has a common shorter form. Aliases must be unique and not the same as any
title.

*  Tags can also be specified in the YAML metadata block. They can be referenced in a page using
the following syntax: '{{tag1}}'. This will produce a list of page links that have the "tag1" tag.
Lists of pages can be created by combining tags in various ways:
	*  {{tag1 tag2}} lists all pages with 'tag1' or 'tag2'
	*  {{tag1 +tag2}} lists all pages with 'tag1' and 'tag2'
	*  {{tag1 -tag2}} lists all pages with 'tag1' that do not have 'tag2'
	*  {{*}} list all pages that have any tag at all

The files in the specified output folder are named according to their title (not their input file
name). For example, a page with the "title" metadata of "A Page Title" will be converted to
"a_page_title.md". Output files can then be processed using a Markdown processor (the assumptions
is that pandoc is being used).

"""

import os
import re
import sys
import json

###

def create_indexes():

	for file in file_list:

		with open(os.path.join(source_dir, file), "r") as input_file:
			contents = input_file.read()

		# get title
		title = parse_metadata("title", contents)

		if not title:
			continue

		if title not in index_titles:
			index_titles[title] = create_valid_filename(title)
		else:
			print sys.argv[0] + ": skipping '" + file + "', duplicate title '" + title + "'"
			continue

		# get alias (if any)
		alias = parse_metadata("alias", contents)

		if alias:
			if alias not in index_alias and alias not in index_titles:
				index_alias[alias] = title
			else:
				print sys.argv[0] + ": duplicate alias '" + alias + "' in file '" + file + "'"

		# get tags (if any)
		tags = parse_metadata("tags", contents)

		if tags:
			# if there are tags remove spaces and brackets then split into list
			tags = tags.translate(None, ' []').split(",")
		else:
			continue

		# add each tag to index, with titles as set
		for tag in tags:
			if tag not in index_tags:
				index_tags[tag] = set()

			index_tags[tag].add(title)


###

def update_search_index(contents):
	# at this point must have a title
	terms = parse_metadata("title", contents)

	alias = parse_metadata("alias", contents)
	if alias:
		terms += " " + alias

	summary = parse_metadata("summary", contents)
	if summary:
		terms += " " + summary

	tags = parse_metadata("tags", contents)
	if tags:
		terms += " " + tags

	keywords = parse_metadata("keywords", contents)
	if keywords:
		terms += " " + keywords

	# remove punctuation etc from YAML values, make lower case
	terms = terms.translate(None, '[]').replace(",", " ").strip().lower().split(" ")

	# remove stop words
	terms = [word for word in terms if word not in index_stopwords]

	# update search index, use unique terms only (set() removes duplicates)
	search = { "file" : index_titles[title], "title" : title, "terms" : ' '.join(str(s) for s in set(terms)) }
	index_search.append(search)


###

def convert_page_link(page):
	# format is [[Page name]] or [[Show name|Page name]]

	page_name = str(page.group())[2:-2]
	show_name = page_name

	if "|" in page_name:
		show_name, page_name = page_name.split("|")

	# resolve any alias for the title
	if page_name in index_alias:
		page_name = index_alias[page_name]

	if page_name in index_titles:
		# if title exists in index make into a link
		return "[" + show_name + "](" + index_titles[page_name] + ".html)"
	else:
		# if title does not exist in index then turn into bracketed span with class='broken'
		return "[" + page_name + "]{.broken}"


###

def convert_tag_link(tags):
	tag_list = str(tags.group())[2:-2]
	tag_list = tag_list.split(" ")
	page_set = set()

	# get initial category
	tag_name = tag_list[0]

	# check that first tag value exists
	if tag_name not in index_tags:

		# if the first tag is "*" then list all pages; otherwise return empty string
		if "*" in tag_name:
			page_set = set(index_titles.keys())
		else:
			return

	else:
		# copy first tag set
		page_set = set(index_tags[tag_list[0]])

		# add other categories
		for __, tag in enumerate(tag_list[1:]):

			if tag[0] == '+' or tag[0] == '-':
				tag_name = tag[1:]
			else:
				tag_name = tag

			if tag_name not in index_tags:
				continue

			if tag[0] == '+':
				page_set = page_set & index_tags[tag_name]
			elif tag[0] == '-':
				page_set = page_set - index_tags[tag_name]
			else:
				page_set = page_set | index_tags[tag_name]

	# format the set into a string of page links, sorted alphabetically
	page_list = ""

	for page in sorted(list(page_set)):
		page_list += "[[" + page + "]]\n\n"

	return page_list


###

def create_valid_filename(s):
	# see also https://stackoverflow.com/questions/295135/turn-a-string-into-a-valid-filename
	# re: Django comment about removing unicode chars
	s = str(s).strip().replace(' ', '_').lower()
	return re.sub(r'(?u)[^-\w.]', '', s)


###

def parse_metadata(metadata, contents):
	if metadata not in regex_meta:
		return None

	value = regex_meta[metadata].search(contents)

	if value:
		value = value.group(1).strip()

	return value


### MAIN ###

if len(sys.argv) != 3:
	print "usage: " + sys.argv[0] + " source_dir target_dir"
	exit()

source_dir = sys.argv[1]
target_dir = sys.argv[2]

# prepare regular expressions

regex_meta = {}
regex_meta["title"] = re.compile(r"[T|t]itle:(.*)[\r\n|\r|\n]")
regex_meta["alias"] = re.compile(r"[A|a]lias:(.*)[\r\n|\r|\n]")
regex_meta["tags"] = re.compile(r"[T|t]ags:(.*)[\r\n|\r|\n]")
regex_meta["keywords"] = re.compile(r"[K|k]eywords:(.*)[\r\n|\r|\n]")
regex_meta["summary"] = re.compile(r"[S|s]ummary:(.*)[\r\n|\r|\n]")

regex_page_link = re.compile(r"\[\[[\w\s,|'-]*\]\]")
regex_tags_link = re.compile(r"\{\{[\w\s\*+-]*\}\}")

# set up indexes

index_tags = {}		# index of tags, containing set of titles with that tag
index_titles = {}	# index of titles, containing associated base file name
index_alias = {}	# index of title aliases (one per page only, must be unique and not in a title)

index_search = []	# index for JSON search terms
index_stopwords = ['a', 'an', 'and', 'be', 'by', 'i', 'it', 'is', 'the']

# get list of Markdown files

file_list = [file for file in os.listdir(source_dir) if re.match(r"[\w]+.*\.md", file)]

# create indexes

create_indexes()

# process files

for file in file_list:

	with open(os.path.join(source_dir, file), "r") as input_file:
		contents = input_file.read()

	title = parse_metadata("title", contents)

	if not title:
		print sys.argv[0] + ": skipping '" + file + "', no title found"
		continue

	# replace tags links first (this may create page links)
	contents = regex_tags_link.sub(convert_tag_link, contents)

	# replace page links
	contents = regex_page_link.sub(convert_page_link, contents)

	# get output file name by adding ".md" to title's file name
	with open(os.path.join(target_dir, index_titles[title] + ".md"), "w") as output_file:
		output_file.write(contents)

	# add terms to search index
	update_search_index(contents)

# write search index

with open(os.path.join(target_dir, "index.json"), "w") as json_file:
	json.dump(index_search, json_file, indent=4)

