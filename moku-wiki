#!/usr/bin/env python

""" moku-wiki

Converts a folder of Markdown files, applying the following transforms:

*  Inter-page links can be specified using the target page's title (as specified in its YAML
metadata block), e.g. '[[A Page Title]]'. This is converted to a standard Markdown link to the
HTML version of that page: '[A Page Title](a_page_title.html)'.

*  The YAML metadata can also have an "alias" field which can be used to link to that page instead
of the title. This can be useful if the actual title that is to be displayed (the "formal" title,
if you will) is long but has a common shorter form. Aliases must be unique and not the same as any
title.

*  Tags can also be specified in the YAML metadata block. They can be referenced in a page using
the following syntax: '{{tag1}}'. This will produce a list of page links that have the "tag1" tag.
Lists of pages can be created by combining tags in various ways:
	*  {{tag1 tag2}} lists all pages with 'tag1' or 'tag2'
	*  {{tag1 +tag2}} lists all pages with 'tag1' and 'tag2'
	*  {{tag1 -tag2}} lists all pages with 'tag1' that do not have 'tag2'
	*  {{*}} list all pages that have any tag at all
	*  {{#}} the total number of pages
	*  {{#tag}} the total number of pages with 'tag'
	*  {{@}} a list of all tags (bracketed spans with a class of 'tag')

The files in the specified output folder are named according to their title (not their input file
name). For example, a page called "file1.md" with the "title" metadata equal to "A Page Title" will
be converted to "a_page_title.md". Output files can then be processed using a Markdown processor
(the assumption is that pandoc is being used).

Using the '--index' option will also output an "index.json" file that contains a JSON object
suitable for use in a webpage.

"""

import os
import re
import json
import string
import argparse

###

def create_indexes():
	# create all indexes

	for file in file_list:

		with open(os.path.join(args.source, file), "r") as input_file:
			contents = input_file.read()

		# get title
		title = parse_metadata("title", contents)

		if not title:
			continue

		if title not in index["title"]:
			index["title"][title] = create_valid_filename(title)
		else:
			print "moku-wiki: skipping '" + file + "', duplicate title '" + title + "'"
			continue

		# get alias (if any)
		alias = parse_metadata("alias", contents)

		if alias:
			if alias not in index["alias"] and alias not in index["title"]:
				index["alias"][alias] = title
			else:
				print "moku-wiki: duplicate alias '" + alias + "' in file '" + file + "'"

		# get tags (if any)
		tags = parse_metadata("tags", contents)

		if tags:
			# if there are tags remove brackets and split into list
			tags = tags[1:-1].split(",")
		else:
			continue

		# add each tag to index, with titles as set
		for tag in tags:

			# strip end spaces and convert to lower case
			tag_name = tag.strip().lower()

			if tag_name not in index["tags"]:
				index["tags"][tag_name] = set()

			index["tags"][tag_name].add(title)

###

def update_search_index(contents):
	# update the search index with strings extracted from metadata

	# at this point must have a title
	terms = parse_metadata("title", contents)

	alias = parse_metadata("alias", contents)

	if alias:
		terms += " " + alias

	summary = parse_metadata("summary", contents)

	if summary:
		terms += " " + summary

	tags = parse_metadata("tags", contents)

	if tags:
		terms += " " + tags[1:-1]

	keywords = parse_metadata("keywords", contents)

	if keywords:
		terms += " " + keywords[1:-1]

	# remove punctuation etc from YAML values, make lower case
	table = string.maketrans(",;_()", "     ")
	terms = terms.translate(table).lower().split()

	# remove stop words
	terms = [word for word in terms if word not in stop_list]

	# update search index, use unique terms only (set() removes duplicates)
	search = { "file" : index["title"][title],
			   "title" : title,
			   "terms" : ' '.join(str(s) for s in set(terms)) }

	index["search"].append(search)


###

def convert_page_link(page):
	# return a page link constructed from the given page title

	# usual format is [[Page name]] or [[Show name|Page name]]
	# with namespaces format is [[ns:Page name]] or [[Show name|ns:Page name]]

	page_name = str(page.group())[2:-2]
	show_name = ""

	if "|" in page_name:
		show_name, page_name = page_name.split("|")

	# resolve namespace
	namespace = ""

	if ":" in page_name:
		namespace, page_name = page_name.rsplit(":", 1)

		namespace = namespace.replace(":","/") + "/"

		if args.namespace == "simple":
			namespace = "../" + namespace

	# set show name if not already done
	if not show_name:
		show_name = page_name

	# resolve any alias for the title
	if page_name in index["alias"]:
		page_name = index["alias"][page_name]

	page_link = ""

	if page_name in index["title"]:
		# if title exists in index make into a link
		page_link = "[" + show_name + "](" + index["title"][page_name] + ".html)"

	else:
		if namespace:
			# title not in index but namespace set, make up link on the fly
			page_link = "[" + show_name + "](" + namespace + create_valid_filename(page_name) + ".html)"
		else:
			# if title does not exist in index then turn into bracketed span with class='broken' (default)
			page_link = "[" + page_name + "]{." + args.broken + "}"

	return page_link


###

def convert_tags_link(tags):
	# return a string containing a list of page links derived from a tag specification

	tag_list = str(tags.group())[2:-2]
	tag_list = tag_list.split()

	# get initial category
	tag_name = tag_list[0].replace("_"," ").lower()
	tag_links = ""

	# check that first tag value exists
	if tag_name not in index["tags"]:

		# check for special characters
		if "*" in tag_name:
			# if the first tag contains a "*" then list all pages
			tag_links = "[[" + "]]\n\n[[".join(sorted(index["title"].keys())) + "]]\n\n"

		elif "@" in tag_name:
			# if the first tag contains a "@" then list all tags as bracketed span with class='tag' (default)
			tag_class = "]{." + args.tag + "}\n\n["
			tag_links = "[" + tag_class.join(sorted(index["tags"].keys())) + "]{." + args.tag + "}"

		elif "#" in tag_name:
			# if the first tag contains a "#" then return the count of pages

			if tag_name == "#":
				# a single "#" returns total number of pages
				tag_links = str(len(index["title"].keys()))
			else:
				# the string "#tag" returns number of pages with that tag
				if tag_name[1:] in index["tags"]:
					tag_links = str(len(index["tags"][tag_name[1:]]))

	else:
		# copy first tag set
		page_set = set(index["tags"][tag_name])

		# add other categories
		for __, tag in enumerate(tag_list[1:]):

			if tag[0] == '+' or tag[0] == '-':
				tag_name = tag[1:]
			else:
				tag_name = tag

			# normalise tag name
			tag_name = tag_name.replace("_"," ").lower()

			if tag_name not in index["tags"]:
				continue

			if tag[0] == '+':
				page_set = page_set & index["tags"][tag_name]
			elif tag[0] == '-':
				page_set = page_set - index["tags"][tag_name]
			else:
				page_set = page_set | index["tags"][tag_name]

		# format the set into a string of page links, sorted alphabetically
		tag_links = "[[" + "]]\n\n[[".join(sorted(page_set)) + "]]\n\n"

	# return list of tag links
	return tag_links


###

def create_valid_filename(name):
	# return a valid filename from the given name

	# see also https://stackoverflow.com/questions/295135/turn-a-string-into-a-valid-filename and Django comment about removing unicode chars
	name = str(name).strip().replace(' ', '_').lower()
	return re.sub(r'(?u)[^-\w.]', '', name)


###

def parse_metadata(metadata, contents):
	# return the contents for the given metadata string, using the global regex's

	if metadata not in regex_meta:
		return None

	value = regex_meta[metadata].search(contents)

	if value:
		value = value.group(1).strip()

	return value


### MAIN ###

parser = argparse.ArgumentParser(description="Convert folder of Markdown files to support interpage linking and tags")
parser.add_argument("source", help="Source directory")
parser.add_argument("target", help="Target directory")
parser.add_argument("-i", "--index", help="Produce search index", action="store_true")
parser.add_argument("-n", "--namespace", choices=["simple", "full"], default="simple", help="Namespace complexity (default is 'simple')")
parser.add_argument("-b", "--broken", default="broken", help="CSS class for broken links (default is 'broken')")
parser.add_argument("-t", "--tag", default="tag", help="CSS class for tag links (default is 'tag')")
args = parser.parse_args()

# prepare regular expressions

regex_meta = {}
regex_meta["title"] = re.compile(r"[T|t]itle:(.*)[\r\n|\r|\n]")
regex_meta["alias"] = re.compile(r"[A|a]lias:(.*)[\r\n|\r|\n]")
regex_meta["tags"] = re.compile(r"[T|t]ags:(.*)[\r\n|\r|\n]")
regex_meta["keywords"] = re.compile(r"[K|k]eywords:(.*)[\r\n|\r|\n]")
regex_meta["summary"] = re.compile(r"[S|s]ummary:(.*)[\r\n|\r|\n]")

regex_link = {}
regex_link["page"] = re.compile(r"\[\[[\w\s,.:|'-]*\]\]")
regex_link["tags"] = re.compile(r"\{\{[\w\s\*#@'+-]*\}\}")

# set up indexes

index = {}
index["tags"] = {}		# index of tags, containing set of titles with that tag
index["title"] = {}		# index of titles, containing associated base file name
index["alias"] = {}		# index of title aliases (one per page only, must be unique and not in a title)
index["search"] = []	# index for JSON search terms

# list of stop words for search indexing

stop_list = ['a', 'an', 'and', 'be', 'by', 'i', 'it', 'is', 'the']

# get list of Markdown files

file_list = [file for file in os.listdir(args.source) if re.match(r"[\w]+.*\.md", file)]

# create indexes

create_indexes()

# process files

for file in file_list:

	with open(os.path.join(args.source, file), "r") as input_file:
		contents = input_file.read()

	title = parse_metadata("title", contents)

	if not title:
		print "moku-wiki: skipping '" + file + "', no title found"
		continue

	# replace tag links first (this may create page links, so do this first)
	contents = regex_link["tags"].sub(convert_tags_link, contents)

	# replace page links
	contents = regex_link["page"].sub(convert_page_link, contents)

	# get output file name by adding ".md" to title's file name
	with open(os.path.join(args.target, index["title"][title] + ".md"), "w") as output_file:
		output_file.write(contents)

	# add terms to search index
	if args.index:
		update_search_index(contents)

# write search index

if args.index:
	with open(os.path.join(args.target, "index.json"), "w") as json_file:
		json.dump(index["search"], json_file, indent=4)

