#!/usr/bin/env python

""" moku-wiki

Converts a folder of Markdown files, applying the following transforms:

*  Inter-page links can be specified using the target page's title (as specified in its YAML
metadata block), e.g. '[[A Page Title]]'. This is converted to a standard Markdown link to the
HTML version of that page: '[A Page Title](a_page_title.html)'.

*  The YAML metadata can also have an "alias" field which can be used to link to that page instead
of the title. This can be useful if the actual title that is to be displayed (the "formal" title,
if you will) is long but has a common shorter form. Aliases must be unique and not the same as any
title.

*  Tags can also be specified in the YAML metadata block. They can be referenced in a page using
the following syntax: '{{tag1}}'. This will produce a list of page links that have the "tag1" tag.
Lists of pages can be created by combining tags in various ways:
	*  {{tag1 tag2}} lists all pages with 'tag1' or 'tag2'
	*  {{tag1 +tag2}} lists all pages with 'tag1' and 'tag2'
	*  {{tag1 -tag2}} lists all pages with 'tag1' that do not have 'tag2'
	*  {{*}} list all pages that have any tag at all
	*  {{#}} the total number of pages
	*  {#tag} the total number of pages with 'tag'

The files in the specified output folder are named according to their title (not their input file
name). For example, a page with the "title" metadata of "A Page Title" will be converted to
"a_page_title.md". Output files can then be processed using a Markdown processor (the assumptions
is that pandoc is being used).

Using the '--index' option will also output an "index.json" file that contains a JSON object
suitable for use in a webpage.

"""

import os
import re
import json
import string
import argparse

###

def create_indexes():

	for file in file_list:

		with open(os.path.join(args.source, file), "r") as input_file:
			contents = input_file.read()

		# get title
		title = parse_metadata("title", contents)

		if not title:
			continue

		if title not in index["titles"]:
			index["titles"][title] = create_valid_filename(title)
		else:
			print "moku-wiki: skipping '" + file + "', duplicate title '" + title + "'"
			continue

		# get alias (if any)
		alias = parse_metadata("alias", contents)

		if alias:
			if alias not in index["alias"] and alias not in index["titles"]:
				index["alias"][alias] = title
			else:
				print "moku-wiki: duplicate alias '" + alias + "' in file '" + file + "'"

		# get tags (if any)
		tags = parse_metadata("tags", contents)

		if tags:
			# if there are tags remove spaces and brackets then split into list
			tags = tags.translate(None, ' []').split(",")
		else:
			continue

		# add each tag to index, with titles as set
		for tag in tags:
			if tag not in index["tags"]:
				index["tags"][tag] = set()

			index["tags"][tag].add(title)


###

def update_search_index(contents):
	# at this point must have a title
	terms = parse_metadata("title", contents)

	alias = parse_metadata("alias", contents)
	if alias:
		terms += " " + alias

	summary = parse_metadata("summary", contents)
	if summary:
		terms += " " + summary

	tags = parse_metadata("tags", contents)
	if tags:
		terms += " " + tags

	keywords = parse_metadata("keywords", contents)
	if keywords:
		terms += " " + keywords

	# remove punctuation etc from YAML values, make lower case
	table = string.maketrans(",_", "  ")
	terms = terms.translate(table, '[]').strip().lower().split(" ")

	# remove stop words
	terms = [word for word in terms if word not in stop_list]

	# update search index, use unique terms only (set() removes duplicates)
	search = { "file" : index["titles"][title], "title" : title, "terms" : ' '.join(str(s) for s in set(terms)) }
	index["search"].append(search)


###

def convert_page_link(page):
	# format is [[Page name]] or [[Show name|Page name]]
	# with namespaces [[namespace:Page name]] or [[Show name|namespace:Page name]]

	page_name = str(page.group())[2:-2]
	show_name = ""

	if "|" in page_name:
		show_name, page_name = page_name.split("|")

	# resolve namespace
	namespace = ""

	if ":" in page_name:
		namespace, page_name = page_name.rsplit(":", 1)

		if not show_name:
			show_name = page_name

		namespace = namespace.replace(":","/") + "/"

	# resolve any alias for the title
	if page_name in index["alias"]:
		page_name = index["alias"][page_name]

	page_link = ""

	if page_name in index["titles"]:
		# if title exists in index make into a link
		page_link = "[" + show_name + "](" + index["titles"][page_name] + ".html)"
	else:
		if namespace:
			# title not in index but namespace set, make up link on the fly
			page_link = "[" + show_name + "](" + namespace + create_valid_filename(page_name) + ".html)"
		else:
			# if title does not exist in index then turn into bracketed span with class='broken'
			page_link = "[" + page_name + "]{.broken}"

	return page_link

###

def convert_tag_link(tags):
	tag_list = str(tags.group())[2:-2]
	tag_list = tag_list.split(" ")
	page_set = set()

	# get initial category
	tag_name = tag_list[0]

	# check that first tag value exists
	if tag_name not in index["tags"]:

		# check for special characters
		if "*" in tag_name:
			# if the first tag contains a "*" then list all pages
			page_set = set(index["titles"].keys())
		elif "#" in tag_name:
			# if the first tag contains a "#" then return the count
			if tag_name == "#":
				return str(len(index["titles"].keys()))
			else:
				return str(len(set(index["tags"][tag_name[1:]])))
		else:
			# otherwise return empty string
			return

	else:
		# copy first tag set
		page_set = set(index["tags"][tag_list[0]])

		# add other categories
		for __, tag in enumerate(tag_list[1:]):

			if tag[0] == '+' or tag[0] == '-':
				tag_name = tag[1:]
			else:
				tag_name = tag

			if tag_name not in index["tags"]:
				continue

			if tag[0] == '+':
				page_set = page_set & index["tags"][tag_name]
			elif tag[0] == '-':
				page_set = page_set - index["tags"][tag_name]
			else:
				page_set = page_set | index["tags"][tag_name]

	# format the set into a string of page links, sorted alphabetically
	page_list = ""

	for page in sorted(list(page_set)):
		page_list += "[[" + page + "]]\n\n"

	return page_list


###

def create_valid_filename(s):
	# see also https://stackoverflow.com/questions/295135/turn-a-string-into-a-valid-filename
	# re: Django comment about removing unicode chars
	s = str(s).strip().replace(' ', '_').lower()
	return re.sub(r'(?u)[^-\w.]', '', s)


###

def parse_metadata(metadata, contents):
	if metadata not in regex_meta:
		return None

	value = regex_meta[metadata].search(contents)

	if value:
		value = value.group(1).strip()

	return value


### MAIN ###

parser = argparse.ArgumentParser(description="Convert folder of Markdown files to support interpage linking and tags")
parser.add_argument("source", help="Source directory")
parser.add_argument("target", help="Target directory")
parser.add_argument("-i", "--index", help="Produce search index", action="store_true")
args = parser.parse_args()

# prepare regular expressions

regex_meta = {}
regex_meta["title"] = re.compile(r"[T|t]itle:(.*)[\r\n|\r|\n]")
regex_meta["alias"] = re.compile(r"[A|a]lias:(.*)[\r\n|\r|\n]")
regex_meta["tags"] = re.compile(r"[T|t]ags:(.*)[\r\n|\r|\n]")
regex_meta["keywords"] = re.compile(r"[K|k]eywords:(.*)[\r\n|\r|\n]")
regex_meta["summary"] = re.compile(r"[S|s]ummary:(.*)[\r\n|\r|\n]")

regex_link = {}
regex_link["page"] = re.compile(r"\[\[[\w\s,.:|'-]*\]\]")
regex_link["tags"] = re.compile(r"\{\{[\w\s\*#+-]*\}\}")

# set up indexes

index = {}
index["tags"] = {}		# index of tags, containing set of titles with that tag
index["titles"] = {}	# index of titles, containing associated base file name
index["alias"] = {}		# index of title aliases (one per page only, must be unique and not in a title)
index["search"] = []	# index for JSON search terms

stop_list = ['a', 'an', 'and', 'be', 'by', 'i', 'it', 'is', 'the']

# get list of Markdown files

file_list = [file for file in os.listdir(args.source) if re.match(r"[\w]+.*\.md", file)]

# create indexes

create_indexes()

# process files

for file in file_list:

	with open(os.path.join(args.source, file), "r") as input_file:
		contents = input_file.read()

	title = parse_metadata("title", contents)

	if not title:
		print "moku-wiki: skipping '" + file + "', no title found"
		continue

	# replace tags links first (this may create page links)
	contents = regex_link["tags"].sub(convert_tag_link, contents)

	# replace page links
	contents = regex_link["page"].sub(convert_page_link, contents)

	# get output file name by adding ".md" to title's file name
	with open(os.path.join(args.target, index["titles"][title] + ".md"), "w") as output_file:
		output_file.write(contents)

	# add terms to search index
	if args.index:
		update_search_index(contents)

# write search index

if args.index:
	with open(os.path.join(args.target, "index.json"), "w") as json_file:
		json.dump(index["search"], json_file, indent=4)

